{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "import cv2\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Flatten, BatchNormalization, GlobalAveragePooling2D, Conv2D, MaxPooling2D\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt3\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dataset link: https://www.kaggle.com/datasets/ninadaithal/imagesoasis\n",
    "\n",
    "# Dataset path and class names\n",
    "dataset_path = 'OASIS Data'\n",
    "classes = ['Non Demented', 'Mild Dementia', 'Moderate Dementia', 'Very mild Dementia']\n",
    "image_paths = []\n",
    "labels = []\n",
    "\n",
    "num_files = 1464  # Number of files to select randomly from each category (number was chosen because there are only 488 images for moderate dementia)\n",
    "\n",
    "# Function to load images\n",
    "def load_images(paths, img_size=(224, 224)):\n",
    "    images = []\n",
    "    for path in paths:\n",
    "        # Read image (grayscale for simplicity)\n",
    "        img = cv2.imread(path, cv2.IMREAD_GRAYSCALE)\n",
    "        img = cv2.resize(img, img_size)\n",
    "        img = img / 255.0  # Normalize to [0, 1]\n",
    "        images.append(img)\n",
    "    return np.array(images)\n",
    "\n",
    "# Iterate through classes and load images\n",
    "non_demented_path = os.path.join(dataset_path, 'Non Demented')\n",
    "non_demented_files = os.listdir(non_demented_path)\n",
    "non_demented_files = random.sample(non_demented_files, min(num_files, len(non_demented_files)))\n",
    "\n",
    "for image_filename in non_demented_files:\n",
    "    image_path = os.path.join(non_demented_path, image_filename)\n",
    "    if os.path.isfile(image_path):\n",
    "        image_paths.append(image_path)\n",
    "        labels.append(0)  # Label for Non Demented\n",
    "\n",
    "dementia_classes = ['Mild Dementia', 'Moderate Dementia', 'Very mild Dementia']\n",
    "for category in dementia_classes:\n",
    "    category_path = os.path.join(dataset_path, category)\n",
    "    category_files = os.listdir(category_path)\n",
    "    selected_files = random.sample(category_files, min(num_files // len(dementia_classes), len(category_files)))\n",
    "    for image_filename in selected_files:\n",
    "        image_path = os.path.join(category_path, image_filename)\n",
    "        if os.path.isfile(image_path):\n",
    "            image_paths.append(image_path)\n",
    "            labels.append(1)  # Label for Dementia (combined)\n",
    "\n",
    "# Convert labels to numpy array\n",
    "labels = np.array(labels)\n",
    "\n",
    "# Load the images\n",
    "X = load_images(image_paths)\n",
    "\n",
    "# Extract subject ID\n",
    "subjects = [image_path.split(os.path.sep)[-1].split('OAS1_')[1].split('_')[0] for image_path in image_paths]\n",
    "\n",
    "# Perform train/test split by subject to prevent data leakage\n",
    "unique_subjects = list(set(subjects))\n",
    "train_subjects, val_subjects = train_test_split(unique_subjects, test_size=0.3, random_state=42)\n",
    "\n",
    "# Split images based on subject IDs\n",
    "train_indices = [i for i, subject in enumerate(subjects) if subject in train_subjects]\n",
    "val_indices = [i for i, subject in enumerate(subjects) if subject in val_subjects]\n",
    "X_train = X[train_indices]\n",
    "y_train = labels[train_indices]\n",
    "X_val = X[val_indices]\n",
    "y_val = labels[val_indices]\n",
    "\n",
    "# Reshape to include channel dimension (grayscale)\n",
    "X_train = X_train[..., np.newaxis]\n",
    "X_val = X_val[..., np.newaxis]\n",
    "\n",
    "# Normalize the images\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1./255,         \n",
    ")\n",
    "\n",
    "# Validation data does not require augmentation, only rescaling\n",
    "val_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "# Apply augmentation only to the training data\n",
    "train_generator = train_datagen.flow(X_train, y_train, batch_size=32)\n",
    "val_generator = val_datagen.flow(X_val, y_val, batch_size=32)\n",
    "\n",
    "\n",
    "#Basic model \n",
    "model = Sequential([\n",
    "    Conv2D(32, (3, 3), activation='relu', input_shape=(224, 224, 1)),\n",
    "    MaxPooling2D(pool_size=(2, 2)),\n",
    "\n",
    "    Conv2D(64, (3, 3), activation='relu'),\n",
    "    MaxPooling2D(pool_size=(2, 2)),\n",
    "\n",
    "    Flatten(),\n",
    "    Dense(128, activation='relu'),\n",
    "\n",
    "    Dropout(0.5),\n",
    "    Dense(1, activation='sigmoid')  \n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "model.fit(train_generator, validation_data=val_generator, epochs=20)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
